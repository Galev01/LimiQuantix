# limiquantix Virtualization Platform
## "The VMware Killer"

**Vision:** Build a complete, modern replacement for VMware vSphere that includes the hypervisor host OS, control plane, guest agent, storage, and networking.

---

## 1. Executive Summary

limiquantix is a **distributed, cloud-native virtualization platform** designed to replace VMware vSphere. The system prioritizes:

- **Simplicity**: 5-minute cluster setup (vs. days for VMware)
- **Performance**: <1% platform overhead
- **Modern UX**: Consumer-grade dashboard (Vercel/Linear aesthetics)
- **API-First**: Every feature is an API call first

This fills the market gap created by Broadcom's VMware acquisition, targeting enterprises seeking:
- Lower cost than Nutanix
- Less complexity than OpenStack
- More features than Proxmox

---

## 2. Platform Components

### Complete VMware Replacement Map

| VMware Component | limiquantix Equivalent | Status |
|------------------|------------------------|--------|
| **vSphere Web Client** | React Dashboard | ‚úÖ 99% |
| **vCenter Server** | Go Control Plane | ‚úÖ 92% |
| **ESXi Host Agent** | Rust Node Daemon | ‚úÖ 90% |
| **VMware Tools** | Rust Guest Agent | ‚úÖ 85% |
| **vSAN / VMFS** | Ceph / LINSTOR | ‚úÖ 80% |
| **NSX-T / vDS** | OVN / OVS (QuantumNet) | ‚è≥ 15% |
| **ESXi OS** | limiquantix OS | ‚ùå 0% |
| **vMotion** | Live Migration | ‚è≥ 50% |
| **HA / DRS** | HA Manager / DRS Engine | ‚úÖ Done |

---

## 3. Current Implementation Status

### ‚úÖ Phase 1: Control Plane Foundation (COMPLETE)

| Component | Status | Description |
|-----------|--------|-------------|
| Frontend (React) | ‚úÖ 99% | 16 pages, Image Library, ISO upload, password/SSH auth, VM actions |
| Backend (Go) | ‚úÖ 92% | All services, scheduler, HA, DRS, storage backends |
| Proto/API | ‚úÖ 100% | Compute, Storage, Network + Cloud-Init + Guest Agent |
| Node Daemon (Rust) | ‚úÖ 90% | gRPC, cloud-init ISO, backing files, real VM creation |
| Guest Agent (Rust) | ‚úÖ 85% | Linux/Windows, telemetry, script execution, file browser |
| Hypervisor Abstraction | ‚úÖ 100% | Mock + Libvirt + Cloud Image backends |
| Storage Backends | ‚úÖ 80% | Local, NFS, Ceph RBD, iSCSI with LVM thin provisioning |
| Frontend ‚Üî Backend | ‚úÖ 100% | API integration complete, cloud-init support |
| Backend ‚Üî Node Daemon | ‚úÖ 98% | Full VM lifecycle, cloud-init provisioning |

### ‚úÖ Phase 2: Real Hypervisor (MOSTLY COMPLETE)

| Component | Status | Notes |
|-----------|--------|-------|
| Linux test environment | ‚úÖ Done | Ubuntu laptop with KVM/libvirt |
| Node Daemon on Linux | ‚úÖ Done | Builds and runs with --features libvirt |
| Node Registration | ‚úÖ Done | Real hardware info sent to control plane |
| Libvirt Backend | ‚úÖ Done | VM creation, domain XML, lifecycle |
| Cloud-Init ISO | ‚úÖ Done | NoCloud datasource with genisoimage |
| Cloud Image Support | ‚úÖ Done | QCOW2 backing file overlays |
| VM Creation (real) | ‚úÖ Done | Full stack: UI ‚Üí Backend ‚Üí Node Daemon ‚Üí Libvirt |
| SSH Key Injection | ‚úÖ Done | Via cloud-init user-data |
| Console Access | ‚úÖ 100% | Web Console (noVNC) + QVMRC Native Client |
| Snapshots | ‚è≥ API ready | Test with libvirt |

### ‚úÖ Phase 3: Guest Agent (COMPLETE)

| Component | Status | Description |
|-----------|--------|-------------|
| Guest Agent Binary | ‚úÖ Done | Rust binary for Linux/Windows |
| Virtio-serial Transport | ‚úÖ Done | Communication channel (no network) |
| Telemetry | ‚úÖ Done | Real memory/disk/CPU usage from inside guest |
| Command Execution | ‚úÖ Done | Run scripts inside VM with user context |
| File Operations | ‚úÖ Done | File browser, upload/download |
| Graceful Shutdown | ‚úÖ Done | Coordinate with host |
| Filesystem Quiescing | ‚úÖ Done | fsfreeze (Linux) / VSS (Windows) |
| Network Configuration | ‚úÖ Done | Netplan, NetworkManager, netsh |
| Cloud-Init Integration | ‚úÖ Done | Auto-install agent during VM creation |

### ‚úÖ Phase 4: Storage Backend (MOSTLY COMPLETE)

| Component | Status | Description |
|-----------|--------|-------------|
| Local Backend | ‚úÖ Done | qemu-img for local directories |
| NFS Backend | ‚úÖ Done | mount + qemu-img for NFS shares |
| Ceph RBD Backend | ‚úÖ Done | rbd CLI for Ceph distributed storage |
| iSCSI Backend | ‚úÖ Done | iscsiadm + LVM thin provisioning |
| Volume Provisioning | ‚úÖ Done | Create/delete/resize volumes |
| Snapshot Storage | ‚úÖ Done | Snapshot disk images |
| Clone (CoW) | ‚úÖ Done | Copy-on-write cloning |
| Frontend Storage UI | ‚úÖ Done | Storage pools + volumes pages |

### ‚ùå Phase 5: Remaining Work

| Component | Effort | Priority |
|-----------|--------|----------|
| **QuantumNet (OVN/OVS)** | 4-6 weeks | **P0** - In Progress |
| limiquantix OS | 8-12 weeks | P1 |

---

## 4. Technical Architecture

### 4.1 System Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              User Access Layer                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                    limiquantix Dashboard (React)                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                       http://localhost:5174                             ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                              HTTP / Connect-RPC
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Control Plane (Go)                                 ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  VMService   ‚îÇ  ‚îÇ NodeService  ‚îÇ  ‚îÇ  Scheduler   ‚îÇ  ‚îÇ  HA Manager  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ  (spread/    ‚îÇ  ‚îÇ  (failover)  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  CRUD +      ‚îÇ  ‚îÇ  Register +  ‚îÇ  ‚îÇ   pack)      ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  Lifecycle   ‚îÇ  ‚îÇ  Heartbeat   ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ         ‚îÇ                                                                    ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                     DaemonPool (Node Daemon Clients)                    ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   etcd       ‚îÇ  ‚îÇ  PostgreSQL  ‚îÇ  ‚îÇ    Redis     ‚îÇ  ‚îÇ   Metrics    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (cluster)   ‚îÇ  ‚îÇ   (state)    ‚îÇ  ‚îÇ   (cache)    ‚îÇ  ‚îÇ  (Prometheus)‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚îÇ
                               gRPC (port 9090)
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         Node Daemon (Rust)                                   ‚îÇ
‚îÇ                       One per hypervisor host                                ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                       NodeDaemonService (gRPC)                          ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  CreateVM ‚îÇ StartVM ‚îÇ StopVM ‚îÇ Snapshots ‚îÇ Migration ‚îÇ Metrics         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                      ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   Telemetry Collector  ‚îÇ  ‚îÇ   Hypervisor  ‚îÇ  ‚îÇ  Registration Client   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ   CPU/Memory/Disk/Net  ‚îÇ  ‚îÇ  Abstraction  ‚îÇ  ‚îÇ  Register + Heartbeat  ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                      ‚îÇ                                       ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ                    ‚ñº                 ‚ñº                 ‚ñº                    ‚îÇ
‚îÇ            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê             ‚îÇ
‚îÇ            ‚îÇ    Mock     ‚îÇ   ‚îÇ   Libvirt   ‚îÇ   ‚îÇ   Cloud     ‚îÇ             ‚îÇ
‚îÇ            ‚îÇ   Backend   ‚îÇ   ‚îÇ   Backend   ‚îÇ   ‚îÇ Hypervisor  ‚îÇ             ‚îÇ
‚îÇ            ‚îÇ    ‚úÖ Done   ‚îÇ   ‚îÇ  ‚è≥ Ready   ‚îÇ   ‚îÇ   ‚ùå Future  ‚îÇ             ‚îÇ
‚îÇ            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                                     ‚îÇ                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                      ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            Linux Kernel (KVM)                                ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                         QEMU / KVM Hypervisor                         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                      ‚îÇ                                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ     VM 1     ‚îÇ  ‚îÇ     VM 2     ‚îÇ  ‚îÇ     VM 3     ‚îÇ  ‚îÇ     VM N     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Guest  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Guest  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Guest  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Guest  ‚îÇ  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Agent  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Agent  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Agent  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ Agent  ‚îÇ  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Rust) ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (Rust) ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (Rust) ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ (Rust) ‚îÇ  ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                        Ceph / LINSTOR Storage                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                              (Shared)                                 ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ                          OVN / OVS Networking                         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ                              (SDN)                                    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 4.2 Data Flow

**VM Creation Flow:**
```
1. User clicks "Create VM" in Dashboard
2. Frontend sends CreateVMRequest via Connect-RPC
3. VMService validates and persists VM
4. Scheduler selects best node (spread/pack strategy)
5. VMService calls Node Daemon via gRPC
6. Node Daemon creates VM via libvirt
7. VM boots, Guest Agent installs
8. Status updates flow back to Dashboard
```

**Node Registration Flow:**
```
1. Node Daemon starts on hypervisor host
2. Collects telemetry (CPU, memory, disks, network)
3. Detects management IP
4. Registers with Control Plane (POST /RegisterNode)
5. Receives server-assigned node ID
6. Starts heartbeat loop (every 30s)
7. Appears in Dashboard as "READY"
```

---

## 5. Implementation Roadmap

### Phase 1: Foundation ‚úÖ COMPLETE
*Duration: 4 weeks (Done)*

- ‚úÖ React Dashboard (15 pages)
- ‚úÖ Go Control Plane (all services)
- ‚úÖ Proto/API definitions
- ‚úÖ Node Daemon (gRPC server)
- ‚úÖ Mock hypervisor
- ‚úÖ Node registration + heartbeat
- ‚úÖ VMService ‚Üí Node Daemon wiring

### Phase 2: Real Hypervisor ‚úÖ MOSTLY COMPLETE
*Duration: 2-3 weeks (Done)*

| Task | Status | Description |
|------|--------|-------------|
| Linux test environment | ‚úÖ Done | Ubuntu laptop with KVM/libvirt |
| Libvirt backend | ‚úÖ Done | VM creation, domain XML, lifecycle |
| Cloud-Init support | ‚úÖ Done | NoCloud ISO generation, auto-provisioning |
| Cloud image support | ‚úÖ Done | QCOW2 backing file overlays |
| Disk image creation | ‚úÖ Done | qemu-img for QCOW2 creation |
| SSH key injection | ‚úÖ Done | Via cloud-init user-data |
| Frontend cloud-init UI | ‚úÖ Done | Image selector, SSH keys, custom config |
| Console proxy | ‚è≥ 50% | VNC info available, WebSocket proxy pending |
| Snapshot testing | ‚è≥ API ready | Test with libvirt |

### Phase 3: Guest Agent ‚úÖ COMPLETE
*Duration: 4-6 weeks (Done)*

| Task | Status | Description |
|------|--------|-------------|
| Virtio-serial transport | ‚úÖ Done | Communication channel (no network) |
| Agent binary | ‚úÖ Done | Rust binary for Linux/Windows guests |
| Telemetry | ‚úÖ Done | Real memory/disk usage from inside guest |
| Command execution | ‚úÖ Done | Run scripts inside VM with user context |
| File operations | ‚úÖ Done | File browser, upload/download files |
| Graceful shutdown | ‚úÖ Done | Coordinate with host |
| Filesystem quiescing | ‚úÖ Done | fsfreeze (Linux) / VSS (Windows) |
| Cloud-init integration | ‚úÖ Done | Auto-install agent during VM creation |
| Windows support | ‚úÖ Done | MSI installer, VSS, netsh |
| Frontend integration | ‚úÖ Done | Agent status, script execution, file browser |

### Phase 4: Storage Backend ‚úÖ MOSTLY COMPLETE
*Duration: 4-6 weeks (Done)*

| Task | Status | Description |
|------|--------|-------------|
| Local backend | ‚úÖ Done | qemu-img for local directories |
| NFS backend | ‚úÖ Done | mount + qemu-img for NFS shares |
| Ceph RBD client | ‚úÖ Done | rbd CLI for distributed block storage |
| iSCSI backend | ‚úÖ Done | iscsiadm + LVM thin provisioning |
| Volume provisioning | ‚úÖ Done | Create/delete/resize volumes |
| Snapshot storage | ‚úÖ Done | Snapshot disk images |
| Clone (CoW) | ‚úÖ Done | Copy-on-write cloning |
| Frontend storage UI | ‚úÖ Done | Storage pools + volumes pages |

### Phase 5: Network Backend (QuantumNet) üöß IN PROGRESS
*Duration: 4-6 weeks*

**Architecture:** OVN (Open Virtual Network) + OVS (Open vSwitch) - The "vDS" for the Modern Era

| Task | Status | Description |
|------|--------|-------------|
| **OVN Northbound Client (Go)** | ‚è≥ | Connect to OVN NB DB via libovsdb |
| **Network Service** | ‚è≥ | CreateNetwork, CreatePort, VLAN/Overlay support |
| **OVS Port Manager (Rust)** | ‚è≥ | Connect VM TAP interfaces to br-int |
| **Libvirt OVS Integration** | ‚è≥ | Generate OVS virtualport XML for VMs |
| **Security Groups (ACLs)** | üìã | Distributed firewall via OVN ACLs |
| **DHCP/DNS** | üìã | Built-in OVN DHCP + CoreDNS Magic DNS |
| **Floating IPs** | üìã | 1:1 NAT via OVN logical routers |
| **Load Balancing** | üìã | L4 load balancing via OVN LB |
| **WireGuard Bastion** | üìã | Direct overlay access from laptops |
| **BGP ToR Integration** | üìã | Enterprise bare-metal integration |

#### Network Types

| Type | VMware Equivalent | Implementation |
|------|-------------------|----------------|
| **VLAN/Flat** | Port Groups | OVN Logical Switch + VLAN tag + localnet port |
| **Overlay/VPC** | NSX Segments | OVN Logical Switch + Geneve encapsulation |
| **External** | Uplink Port Group | Provider network with SNAT |
| **Isolated** | Private Network | No router attachment |

#### Day 2 Features (Strategic Improvements)

| Feature | VMware Way | limiquantix Way (Better) |
|---------|------------|--------------------------|
| **Microsegmentation** | IP-based firewall rules | Tag-based: "Allow Web-Servers ‚Üí DB-Servers" |
| **Floating IPs** | Manual NAT rules | One-click public IP assignment |
| **VPN Access** | NSX Edge (complex) | Built-in WireGuard Bastion |
| **ToR Integration** | Manual VLAN config | BGP auto-advertisement |
| **Magic DNS** | External DNS | `<vm-name>.internal` auto-resolves |

### Phase 6: Host OS üìã PLANNED
*Duration: 8-12 weeks*

| Task | Description |
|------|-------------|
| Base image | Minimal Linux (Alpine/buildroot) |
| Auto-configuration | DHCP, hostname, management network |
| Node Daemon integration | Auto-start, auto-register |
| ISO builder | Generate installable ISO |
| PXE boot | Network boot for bare metal |
| TPM/Secure Boot | Security features |

### Phase 7: Enterprise Features üìã PLANNED
*Duration: 6-8 weeks*

| Task | Description |
|------|-------------|
| Live migration | vMotion equivalent (structure ready) |
| HA testing | Automatic failover |
| DRS testing | Resource balancing |
| Backup engine | VADP equivalent |
| Templates | VM templates and cloning |
| Resource pools | Nested resource allocation |

---

## 6. Priority Matrix

### P0: Critical Path (Must Have)
| Component | Effort | Blocks |
|-----------|--------|--------|
| Real hypervisor testing | 1-2 weeks | Everything |
| Storage (at least LVM) | 2-3 weeks | VM creation |
| Networking (at least bridge) | 2-3 weeks | VM connectivity |
| Guest Agent (basic) | 3-4 weeks | Real VM usage |

### P1: Important (Should Have)
| Component | Effort |
|-----------|--------|
| Live migration | 2 weeks |
| Ceph integration | 3-4 weeks |
| OVN integration | 3-4 weeks |
| Host OS | 8-12 weeks |

### P2: Nice to Have
| Component | Effort |
|-----------|--------|
| Cloud Hypervisor | 4 weeks |
| Backup engine | 4 weeks |
| Multi-tenancy | 3 weeks |

---

## 7. Quick Start

### Run the Full Stack

```bash
# Terminal 1: Control Plane (Go)
cd backend
go run ./cmd/controlplane --dev

# Terminal 2: Node Daemon (Rust)
cd agent
cargo run --bin limiquantix-node -- \
  --dev \
  --listen 127.0.0.1:9090 \
  --control-plane http://127.0.0.1:8080 \
  --register

# Terminal 3: Frontend (React)
cd frontend
npm run dev

# Access Dashboard: http://localhost:5174
```

### Verify Integration

```bash
# Check registered nodes
curl -s -X POST http://127.0.0.1:8080/limiquantix.compute.v1.NodeService/ListNodes \
  -H "Content-Type: application/json" \
  -d '{}' | jq '.nodes[] | {hostname, id, phase: .status.phase}'

# Check health
curl http://127.0.0.1:8080/health
```

---

## 8. Team & Resources

### Recommended Team (Phase 2+)

| Role | Count | Focus |
|------|-------|-------|
| **Lead Architect** | 1 | Architecture, API design |
| **Systems Engineers (Rust)** | 2 | Hypervisor, Guest Agent, Storage |
| **Backend Engineers (Go)** | 2 | Control Plane, Clustering |
| **Frontend Engineer** | 1 | Dashboard, UX |
| **DevOps Engineer** | 1 | CI/CD, Testing, Host OS |

### Infrastructure Needed

| Resource | Purpose |
|----------|---------|
| Linux server with KVM | Real hypervisor testing |
| Ceph cluster (3+ nodes) | Storage testing |
| Network lab | OVN/OVS testing |
| CI runners (bare metal) | Integration tests |

---

## 9. Success Metrics

| Metric | Target | Current |
|--------|--------|---------|
| Time to HA Cluster | < 10 minutes | N/A (no cluster yet) |
| Platform Overhead | < 1% | N/A (testing on Ubuntu laptop) |
| API Response Time | < 100ms | ‚úÖ ~1ms |
| Dashboard FPS | 60fps | ‚úÖ 60fps |
| Node Registration | < 1 second | ‚úÖ ~100ms |
| Heartbeat Interval | 30 seconds | ‚úÖ Working |
| VM Boot Time | < 30 seconds | ‚è≥ Ready to test |
| Cloud-Init Provisioning | < 2 minutes | ‚è≥ Ready to test |
| Live Migration Time | < 10 seconds | N/A |

---

## 10. Next Milestone

**Goal:** ~~Boot a REAL VM via the full stack~~ ‚úÖ ACHIEVED!

**Completed:**
- ‚úÖ Set up Linux hypervisor host with KVM/libvirt (Ubuntu laptop)
- ‚úÖ Deploy Node Daemon with `--features libvirt`
- ‚úÖ Node registers and appears in Dashboard with real hardware info
- ‚úÖ VM creation via Dashboard ‚Üí Backend ‚Üí Node Daemon ‚Üí Libvirt
- ‚úÖ Implemented Node Daemon CreateVM with libvirt domain XML
- ‚úÖ Cloud-init ISO generation (NoCloud datasource)
- ‚úÖ Cloud image support (QCOW2 backing file overlays)
- ‚úÖ SSH key injection via cloud-init
- ‚úÖ Frontend cloud-init UI (image selector, SSH keys, custom config)

**Next Goal:** QuantumNet - Distributed Networking (OVN/OVS integration)

**Currently Implementing (January 3, 2026):**
- üöß OVN Northbound Client (Go) - libovsdb integration
- üöß NetworkService - CreateNetwork/CreatePort with OVN backend
- üöß Rust OVS Port Manager - VM TAP ‚Üí br-int binding
- üöß Libvirt OVS XML - VirtualPort integration

**Completed (January 3, 2026):**
1. ‚úÖ Web Console (noVNC) - Browser-based VNC access
2. ‚úÖ WebSocket VNC Proxy - Control Plane proxies browser ‚Üí VNC
3. ‚úÖ QVMRC Tauri app with full VNC protocol + deep linking
4. ‚úÖ Guest Agent - Full Linux/Windows support with telemetry, scripts, file browser
5. ‚úÖ Storage Backends - Local, NFS, Ceph RBD, iSCSI with LVM thin provisioning
6. ‚úÖ VM Actions Dropdown - Edit settings, resources, run scripts, clone, delete
7. ‚úÖ Cloud-init agent auto-install - Agent installed during VM creation
8. ‚úÖ Image Library - Manage cloud images and ISOs with upload dialog
9. ‚úÖ ISO Upload - Upload ISOs via URL or file with progress tracking
10. ‚úÖ Password/SSH Access - Improved access config with password + SSH keys + validation

**Frontend VM Detail Improvements:**
- ‚úÖ VM Actions Dropdown Menu (Edit Settings, Edit Resources, Run Script, Browse Files, Clone, Delete)
- ‚úÖ Edit Settings Modal (name, description, labels)
- ‚úÖ Edit Resources Modal (CPU cores, memory with presets)
- ‚úÖ Quantix Agent tab with status, script execution, file browser

**Image Library & ISO Upload:**
- ‚úÖ Image Library page (`/storage/images`) with cloud images and ISOs tabs
- ‚úÖ ISOUploadDialog - Upload from URL or file with drag-and-drop
- ‚úÖ ISO catalog with built-in entries (Ubuntu, Debian, Rocky, Windows)
- ‚úÖ Download cloud images from catalog to storage pools
- ‚úÖ Delete images from library

**VM Access Configuration (Cloud-Init):**
- ‚úÖ Password authentication with SSH password enabled (`ssh_pwauth: true`)
- ‚úÖ Password confirmation with validation (match, length >= 8)
- ‚úÖ SSH key validation (format, completeness, duplicate detection)
- ‚úÖ Access summary showing configured methods
- ‚úÖ Warning when no access method configured
- ‚úÖ Using `chpasswd` module for proper password setup
- ‚úÖ PasswordInput component with show/hide toggle

**Immediate Next Steps:**
1. ‚úÖ Complete OVN Northbound Client (Go)
2. ‚úÖ Implement NetworkService with OVN backend
3. ‚úÖ Add OVS port management to Rust Node Daemon
4. üìã Security group enforcement (OVN ACLs)
5. üìã Magic DNS (CoreDNS + OVN state)
6. üìã Floating IPs and NAT

**Estimated Time:** QuantumNet ~4-6 weeks

---

## 11. Documentation

| Document | Path | Description |
|----------|------|-------------|
| VM Model Design | `docs/adr/000001-vm-model-design.md` | VM domain model |
| Node Model Design | `docs/adr/000002-node-model-design.md` | Node domain model |
| Storage Model Design | `docs/adr/000003-storage-model-design.md` | Storage domain |
| Network Model Design | `docs/adr/000004-network-model-design.md` | Network domain |
| gRPC Services | `docs/adr/000005-grpc-services-design.md` | API design |
| Build System | `docs/adr/000006-proto-and-build-system-guide.md` | Proto generation |
| Hypervisor Integration | `docs/adr/000007-hypervisor-integration.md` | Backend decision |
| Node Daemon Plan | `docs/000031-node-daemon-implementation-plan.md` | 6-week roadmap |
| VMService Integration | `docs/000032-vmservice-node-daemon-integration.md` | Service wiring |
| Registration Flow | `docs/000033-node-registration-flow.md` | Node registration |
| Real VM Implementation | `docs/000038-real-vm-implementation.md` | Libvirt VM creation |
| Cloud-Init Provisioning | `docs/000039-cloud-init-provisioning.md` | Cloud-init + cloud images |

---

## 12. Legend

| Symbol | Meaning |
|--------|---------|
| ‚úÖ | Complete |
| ‚è≥ | In Progress |
| üìã | Planned |
| ‚ùå | Not Started / Blocked |
| P0 | Critical priority |
| P1 | High priority |
| P2 | Medium priority |
